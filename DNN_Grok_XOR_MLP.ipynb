{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2W9VAP5pGzc1"
      },
      "outputs": [],
      "source": [
        "## Demo codes for \"Deep Networks Always Grok and Here is Why\", ArXiv 2024\n",
        "## Authors: Ahmed Imtiaz Humayun, Randall Balestriero, Richard Baraniuk\n",
        "## Website: bit.ly/grok-adversarial\n",
        "## Wandb Dashboard containing example logs: bit.ly/grok-adv-trak"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "i44DPD5ZGH6P"
      },
      "outputs": [],
      "source": [
        "#@title License\n",
        "#\n",
        "#The MIT License (MIT)\n",
        "#Copyright © 2024 Ahmed Imtiaz Humayun\n",
        "#Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "#The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqbcj8Tzo7GQ",
        "outputId": "958174b6-5d46-4106-ee63-234b7891e2f2"
      },
      "source": [
        "#@title Setup\n",
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/AhmedImtiazPrio/grok-adversarial.git\n",
        "%cd 'grok-adversarial'\n",
        "\n",
        "!pip install ml_collections\n",
        "!pip install wandb\n",
        "!pip install numba opencv-python\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZeHXkH9_qhAc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/niket/miniconda3/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/niket/grokking/grok-adversarial\n",
            "/home/niket\n",
            "/home/niket/grokking/grok-adversarial\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/niket/miniconda3/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "#@title Imports\n",
        "%cd 'grok-adversarial'\n",
        "\n",
        "import torch as ch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.nn import CrossEntropyLoss, BCELoss\n",
        "from torch.optim import SGD, lr_scheduler, AdamW\n",
        "import numpy as np\n",
        "\n",
        "import ml_collections\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "\n",
        "import wandb\n",
        "\n",
        "\n",
        "from dataloaders import cifar10_dataloaders, cifar10_dataloaders_ffcv, get_LC_samples\n",
        "from models import make_resnet18k\n",
        "from utils import flatten_model, add_hooks_preact_resnet18\n",
        "from attacks import PGD\n",
        "from local_complexity import get_intersections_for_hulls\n",
        "from samplers import get_ortho_hull_around_samples, get_ortho_hull_around_samples_w_orig\n",
        "\n",
        "%cd ../..\n",
        "import splinecam\n",
        "%cd 'grokking/grok-adversarial'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "940J8JeKVPt6"
      },
      "outputs": [],
      "source": [
        "#@title Train and evaluation functions\n",
        "\n",
        "def train(model, loaders, config, add_hook_fn, hulls=None):\n",
        "\n",
        "    model.cuda()\n",
        "\n",
        "    ## setup optimizer\n",
        "    if config.optimizer == 'sgd':\n",
        "        print('Using SGD optimizer')\n",
        "        opt = SGD(model.parameters(),\n",
        "                  lr=config.lr,\n",
        "                  momentum=config.momentum,\n",
        "                  weight_decay=config.weight_decay)\n",
        "\n",
        "    elif config.optimizer == 'adam':\n",
        "        opt = AdamW(model.parameters(),\n",
        "                    lr=config.lr,\n",
        "                    weight_decay=config.weight_decay)\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    ## resume training\n",
        "    if config.resume_step>0 and config.resume_dir is not None:\n",
        "\n",
        "        assert os.path.exists(\n",
        "            os.path.join(\n",
        "                config.load_dir, f'checkpoint-s:{config.resume_step}.pt'\n",
        "                )\n",
        "            ), f\"Resume checkpoint not found\"\n",
        "\n",
        "        base_chkpt = ch.load(os.path.join(config.resume_dir,\n",
        "                             f'checkpoint-s:{-1}.pt'))\n",
        "        model = base_chkpt['model']\n",
        "        opt = base_chkpt['optimizer']\n",
        "        state_chkpt = ch.load(os.path.join(config.resume_dir,\n",
        "                              f'checkpoint-s:{config.resume_step}.pt'))\n",
        "        model.load_state_dict(state_chkpt['model_state_dict'])\n",
        "        opt.load_state_dict(state_chkpt['optimizer_state_dict'])\n",
        "\n",
        "\n",
        "    ### save model and optimizer before training\n",
        "    ### shortcut to avoid removing hooks later\n",
        "    ch.save(\n",
        "              {\n",
        "                  'model': model,\n",
        "                  'optimizer': opt,\n",
        "              },\n",
        "              os.path.join(\n",
        "                  config.model_dir,\n",
        "                  f'checkpoint-s:{-1}.pt'\n",
        "              )\n",
        "    )\n",
        "\n",
        "    iters_per_epoch = len(loaders['train'])\n",
        "    epochs = np.floor(config.num_steps/iters_per_epoch)\n",
        "\n",
        "    if config.lr_schedule_flag:\n",
        "        # Cyclic LR with single triangle\n",
        "        print('Using Learning Rate Schedule')\n",
        "        lr_schedule = np.interp(np.arange((epochs+1) * iters_per_epoch),\n",
        "                                [0, config.lr_peak_epoch * iters_per_epoch, epochs * iters_per_epoch],\n",
        "                                [0, 1, 0])\n",
        "\n",
        "        scheduler = lr_scheduler.LambdaLR(opt, lr_schedule.__getitem__)\n",
        "\n",
        "    loss_fn = ch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "    train_step = 0 if config.resume_step <= 0 else config.resume_step\n",
        "\n",
        "    ## stat dict for plotting convenience\n",
        "    stat_names = ['train_acc','train_loss','test_loss',\n",
        "                  'test_acc','adv_acc', 'train_step', 'l2', 'last_layer_l2'] + \\\n",
        "            [each+'_LC' for each in list(hulls.keys())]\n",
        "\n",
        "    stats = dict(zip(stat_names,[[] for _ in stat_names]))\n",
        "\n",
        "    print(f'Logging stats for steps:{config.log_steps}')\n",
        "\n",
        "    while True:\n",
        "\n",
        "        if train_step > config.num_steps: break\n",
        "\n",
        "        for ims, labs in tqdm(loaders['train'], desc=f\"train_step:{train_step}-{train_step+iters_per_epoch}\"):\n",
        "\n",
        "            ### train step\n",
        "            ims = ims.cuda()\n",
        "            labs = labs.cuda()\n",
        "\n",
        "            if config.use_ffcv and labs.max()>config.num_class-1:\n",
        "              labs = ch.clip(labs,0,config.num_class-1) ## weird ffcv bug\n",
        "\n",
        "            opt.zero_grad()\n",
        "            out = model(ims)\n",
        "\n",
        "            loss = loss_fn(out, labs)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_step += 1\n",
        "\n",
        "            if config.lr_schedule_flag:\n",
        "                scheduler.step()\n",
        "\n",
        "            ### log step\n",
        "            if train_step in config.log_steps:\n",
        "                print('Computing stats...')\n",
        "\n",
        "                model.eval()\n",
        "\n",
        "                ### checkpoint before anything\n",
        "                if config.save_model:\n",
        "\n",
        "                  ch.save(\n",
        "                      {\n",
        "                          'model_state_dict': model.state_dict(),\n",
        "                          'optimizer_state_dict': opt.state_dict(),\n",
        "                      },\n",
        "                      os.path.join(\n",
        "                          config.model_dir,\n",
        "                          f'checkpoint-s:{train_step}.pt'\n",
        "                      )\n",
        "                  )\n",
        "\n",
        "                ## evaluate on train and test\n",
        "                train_acc, train_loss = evaluate(model,\n",
        "                                                 loaders['train'],\n",
        "                                                 loss_fn)\n",
        "                test_acc, test_loss = evaluate(model,loaders['test'],\n",
        "                                                 loss_fn)\n",
        "                l2_norm_in = ch.linalg.norm(model.input_layer.weight).item()\n",
        "                l2_last_layer_norm = ch.linalg.norm(model.output_layer.weight).item()\n",
        "\n",
        "                stats['train_acc'].append(train_acc)\n",
        "                stats['test_acc'].append(test_acc)\n",
        "                stats['train_loss'].append(train_loss)\n",
        "                stats['test_loss'].append(test_loss)\n",
        "                stats['train_step'].append(train_step)\n",
        "                stats['l2'].append(l2_norm_in)\n",
        "                stats['last_layer_l2'].append(l2_last_layer_norm)\n",
        "                \n",
        "                if config.splinecam:\n",
        "                    print('Wrapping model with SplineCam...')\n",
        "                    \n",
        "                    domain = (\n",
        "                    (-config.mu_1 * config.splinecam_domain, config.mu_1 * config.splinecam_domain),\n",
        "                    (config.mu_1 * config.splinecam_domain, config.mu_2 * config.splinecam_domain),\n",
        "                    (config.mu_2 * config.splinecam_domain, -config.mu_2 * config.splinecam_domain),\n",
        "                    (-config.mu_2 * config.splinecam_domain, config.mu_1 * config.splinecam_domain),\n",
        "                    )\n",
        "\n",
        "                    T = splinecam.utils.get_proj_mat(domain)\n",
        "                    NN = splinecam.wrappers.model_wrapper(\n",
        "                        model,\n",
        "                        input_shape=model.input_shape,\n",
        "                        T = T,\n",
        "                        dtype = ch.float64,\n",
        "                        device = 'cuda'\n",
        "                    )\n",
        "\n",
        "                ## evaluate local complexity\n",
        "                # add hooks\n",
        "                if config.compute_LC:\n",
        "\n",
        "                  model, layer_names, activation_buffer = add_hook_fn(model, config)\n",
        "\n",
        "                  if hulls is not None:\n",
        "                    for k in hulls.keys():\n",
        "\n",
        "                      # compute number of neurons that intersect hulls\n",
        "                      # using network activations\n",
        "\n",
        "                      with ch.no_grad():\n",
        "\n",
        "                        n_inters, _ = get_intersections_for_hulls(\n",
        "                                        hulls[k],\n",
        "                                        model=model,\n",
        "                                        batch_size=config.LC_batch_size,\n",
        "                                        layer_names=layer_names,\n",
        "                                        activation_buffer=activation_buffer\n",
        "                                  )\n",
        "\n",
        "                      stats[k+'_LC'].append(n_inters.cpu())\n",
        "\n",
        "                ## evaluate robustness\n",
        "                if config.compute_robust:\n",
        "\n",
        "                  adv_acc = evaluate_adv(model, loaders['test'], config)\n",
        "                  stats['adv_acc'].append(adv_acc)\n",
        "\n",
        "\n",
        "                if config.wandb_log:\n",
        "                  wandb.log({\n",
        "                      'iter' : train_step,\n",
        "                      'train/acc': stats['train_acc'][-1],\n",
        "                      'train/loss': stats['train_loss'][-1],\n",
        "                      'test/acc' : stats['test_acc'][-1],\n",
        "                      'test/loss' : stats['test_loss'][-1],\n",
        "                      'train/LC' : stats['train_LC'][-1].sum(1).mean(0),\n",
        "                      'test/LC' : stats['test_LC'][-1].sum(1).mean(0),\n",
        "                      'random/LC' : stats['rand_LC'][-1].sum(1).mean(0),\n",
        "                      'adv/acc' : stats['adv_acc'][-1],\n",
        "                      'l2' : stats['l2'][-1],\n",
        "                      'last_layer_l2' : stats['last_layer_l2'][-1]\n",
        "                  })\n",
        "\n",
        "                model.train()\n",
        "\n",
        "\n",
        "    ## save after training is complete\n",
        "    ch.save(\n",
        "        {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "        },\n",
        "        os.path.join(\n",
        "            config.model_dir,\n",
        "            f'checkpoint-s:{train_step}.pt'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return stats\n",
        "\n",
        "@ch.no_grad\n",
        "def evaluate(model, dloader, loss_fn=None):\n",
        "\n",
        "  acc = 0\n",
        "  loss = 0\n",
        "  nsamples = 0\n",
        "  nbatch = 0\n",
        "\n",
        "  for ims, labs in dloader:\n",
        "\n",
        "      ims = ims.cuda()\n",
        "      labs = labs.cuda()\n",
        "\n",
        "      outs = model(ims)\n",
        "\n",
        "      if loss_fn is not None:\n",
        "        loss += loss_fn(outs, labs)\n",
        "        nbatch += 1\n",
        "\n",
        "      acc += ch.sum(labs == (outs>0.5)).cpu()\n",
        "      nsamples += outs.shape[0]\n",
        "\n",
        "  return acc/nsamples, loss/nbatch\n",
        "\n",
        "def evaluate_adv(model, dloader, config):\n",
        "\n",
        "  atk = PGD(model,\n",
        "          eps=config.atk_eps,\n",
        "          alpha=config.atk_alpha,\n",
        "          steps=config.atk_itrs,\n",
        "          dmin=config.dmin,\n",
        "          dmax=config.dmax\n",
        "          )\n",
        "\n",
        "  acc = 0\n",
        "  nsamples = 0\n",
        "  for ims, labs in tqdm(dloader, desc=f\"Computing robust acc for eps:{config.atk_eps:.3f}\"):\n",
        "\n",
        "    ims = ims.cuda()\n",
        "    labs = labs.cuda()\n",
        "\n",
        "    adv_images = atk(ims, labs)\n",
        "\n",
        "    with ch.no_grad():\n",
        "        adv_pred = model(adv_images).argmax(dim=-1)\n",
        "\n",
        "    acc += ch.sum(labs == (adv_pred>0.5)).cpu()\n",
        "    nsamples += len(labs)\n",
        "\n",
        "  return acc/nsamples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "itV14eChicBS"
      },
      "outputs": [],
      "source": [
        "def add_hooks_MLP(model, config, verbose=False):\n",
        "    \"\"\"\n",
        "    Add hooks to preact resnet\n",
        "    \"\"\"\n",
        "\n",
        "    names,modules = flatten_model(model)\n",
        "    assert len(names) == len(modules)\n",
        "\n",
        "    ## add hooks to linear layers\n",
        "    layer_ids = np.asarray([i for i,each in enumerate(modules) if (type(each)==ch.nn.Linear)])\n",
        "\n",
        "    activation = {}\n",
        "    def get_activation(name):\n",
        "        def hook(model, input, output):\n",
        "            activation[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    for each in layer_ids:\n",
        "        modules[each].register_forward_hook(get_activation(names[each]))\n",
        "\n",
        "    layer_names = np.sort(np.asarray(names)[layer_ids])\n",
        "\n",
        "    if verbose:\n",
        "        print('Adding Hook to',layer_names)\n",
        "\n",
        "    return model, layer_names, activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hovIKd8b3c_3"
      },
      "outputs": [],
      "source": [
        "def get_config():\n",
        "  \"\"\"hyperparameter configuration.\"\"\"\n",
        "  config = ml_collections.ConfigDict()\n",
        "\n",
        "  config.optimizer = 'sgd'\n",
        "  config.lr = 1e-3\n",
        "  config.momentum = 0.00\n",
        "  config.lr_schedule_flag = False\n",
        "\n",
        "  config.train_batch_size = 100\n",
        "  config.test_batch_size = 100\n",
        "  config.num_steps = 500000                       # number of training steps\n",
        "  config.weight_decay = 0.1\n",
        "  config.label_smoothing = 0.\n",
        "  config.log_steps = np.unique(\n",
        "      np.logspace(0,5.7,50).astype(int).clip(\n",
        "          0,config.num_steps\n",
        "          )\n",
        "      )\n",
        "  config.seed = 42\n",
        "  config.use_aug = False\n",
        "  config.normalize = True                        # rescale cifar10 to have mean 0 std 1.25\n",
        "\n",
        "  if config.normalize:\n",
        "      config.dmax = 2.7537                       # precomputed data max/min needed for PGD\n",
        "      config.dmin = -2.4291\n",
        "  else:\n",
        "      config.dmax = 1\n",
        "      config.dmin = 0\n",
        "\n",
        "\n",
        "  config.save_model = False                      # save every model checkpoint\n",
        "  config.wandb_log = True                        # log using wandb\n",
        "  config.save_model = False                      # save every model checkpoint\n",
        "  config.wandb_log = True                        # log using wandb\n",
        "  config.wandb_proj = 'grok-adv-MLP-XOR'\n",
        "  config.wandb_pref = 'XOR-MLP'\n",
        "  config.use_ffcv = True\n",
        "\n",
        "  ## resnet params\n",
        "  config.k = 16                                  # Resnet width parameter, number of filters in first layer\n",
        "  config.num_class = 10\n",
        "  config.use_bn = False\n",
        "  config.resume_dir = None                       # resume directory absolute path\n",
        "  config.resume_step = -1                        # time step to resume, from resume directory\n",
        "\n",
        "  ## local complexity approx. parameters\n",
        "  config.compute_LC = True\n",
        "  config.approx_n = 256                         # number of samples to use for approximation\n",
        "  config.n_frame = 100                            # number of vertices for neighborhood\n",
        "  config.r_frame = 0.0001                         # radius of \\ell-1 ball neighborhood\n",
        "  config.LC_batch_size = 256\n",
        "  config.inc_centroid = False                     # include original sample as neighborhood vertex\n",
        "\n",
        "\n",
        "  ## adv robustness parameters\n",
        "  config.compute_robust = True                   # note that if normalize==True, data is not bounded between [0,1]\n",
        "  config.atk_eps = 8/255   ## 8/255\n",
        "  config.atk_alpha = 10/255  ## 2/255\n",
        "  config.atk_itrs = 5\n",
        "\n",
        "  config.splinecam = True\n",
        "  config.splinecam_domain = 10\n",
        "\n",
        "  config.input_dimension = 4000\n",
        "  \n",
        "  config.mu_norm = 5\n",
        "  config.train_flip_prob = 0.01\n",
        "  config.dataset_size_4 = 1000\n",
        "  config.test_dataset_size_4 = 1000\n",
        "\n",
        "  config.hidden_dim = 50000\n",
        "\n",
        "  A = np.random.randn( config.input_dimension, 2)\n",
        "  Q, _ = np.linalg.qr(A)\n",
        "  config.mu_1 = Q[:,0] * config.mu_norm\n",
        "  config.mu_2 = Q[:,1] * config.mu_norm\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "config = get_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YtND-11yF3TZ"
      },
      "outputs": [],
      "source": [
        "class XORDataset(ch.utils.data.Dataset):\n",
        "    def __init__(self, mu_1, mu_2, input_dimension, n_samples_per_center, eta_label_flipping_prob):\n",
        "        self.mu_1 = mu_1\n",
        "        self.mu_2 = mu_2\n",
        "\n",
        "        data_1_1 = np.random.multivariate_normal(self.mu_1, np.eye(input_dimension), n_samples_per_center)\n",
        "        data_1_2 = np.random.multivariate_normal(-self.mu_1, np.eye(input_dimension), n_samples_per_center)\n",
        "        data_2_1 = np.random.multivariate_normal(self.mu_2, np.eye(input_dimension), n_samples_per_center)\n",
        "        data_2_2 = np.random.multivariate_normal(-self.mu_2, np.eye(input_dimension), n_samples_per_center)\n",
        "\n",
        "        self.data = np.concatenate([data_1_1, data_1_2, data_2_1, data_2_2])\n",
        "        self.data = ch.Tensor(self.data)\n",
        "        self.labels = ch.concat([ch.zeros(n_samples_per_center*2), ch.ones(n_samples_per_center*2)])\n",
        "\n",
        "        c = ch.Tensor(np.random.rand(n_samples_per_center*4) < eta_label_flipping_prob)\n",
        "        self.labels = c*(1-self.labels) + (1-c)*(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = XORDataset(config.mu_1, config.mu_2, input_dimension = config.input_dimension, n_samples_per_center = config.dataset_size_4, eta_label_flipping_prob = config.train_flip_prob)\n",
        "test_dataset = XORDataset(config.mu_1, config.mu_2, input_dimension = config.input_dimension, n_samples_per_center = config.test_dataset_size_4, eta_label_flipping_prob = 0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hRxZzBqXI0xt"
      },
      "outputs": [],
      "source": [
        "## load data\n",
        "train_loader = ch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=config.train_batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = ch.utils.data.DataLoader(test_dataset,\n",
        "                                           batch_size=config.test_batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "\n",
        "## initialize neighborhood sampler\n",
        "sampler_params = {'n' : config.n_frame if not config.inc_centroid \\\n",
        "                    else config.n_frame+1,\n",
        "                  'r' : config.r_frame, 'seed':config.seed}\n",
        "\n",
        "sampler = get_ortho_hull_around_samples_w_orig if config.inc_centroid \\\n",
        "            else get_ortho_hull_around_samples\n",
        "\n",
        "## select samples for neighborhood computation\n",
        "train_LC_batch, _ = get_LC_samples(train_loader,config)\n",
        "test_LC_batch, _ = get_LC_samples(train_loader,config)\n",
        "if config.normalize:\n",
        "  rand_LC_batch = ch.rand_like(test_LC_batch)*2.8*2 - 2.8\n",
        "else:\n",
        "  rand_LC_batch = ch.rand_like(test_LC_batch)   ## Data domain [0,1]\n",
        "\n",
        "\n",
        "## sample hulls/neighborhoods\n",
        "train_hulls = sampler(\n",
        "  train_LC_batch.cuda(),\n",
        "  **sampler_params\n",
        "    ).cpu()\n",
        "test_hulls = sampler(\n",
        "    test_LC_batch.cuda(),\n",
        "    **sampler_params\n",
        ").cpu()\n",
        "rand_hulls = sampler(\n",
        "    rand_LC_batch.cuda(),\n",
        "    **sampler_params\n",
        ").cpu()\n",
        "\n",
        "\n",
        "## make hull dict. the keys of this dict will be used for logging\n",
        "## you can add hulls separately for different classes as well to\n",
        "## keep track of classwise statistics\n",
        "hulls = {\n",
        "    'train' : train_hulls,\n",
        "    'test' : test_hulls,\n",
        "    'rand' : rand_hulls\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zoX9WBI5ZFvG"
      },
      "outputs": [],
      "source": [
        "loaders = {\n",
        "    'train' : train_loader,\n",
        "    'test' : test_loader\n",
        "}\n",
        "\n",
        "## directory for saving logs and models\n",
        "timestamp = time.ctime().replace(' ','_')\n",
        "config.model_dir = os.path.join(f'./models/{timestamp}')\n",
        "config.log_dir = os.path.join(f'./logs/{timestamp}')\n",
        "os.mkdir(config.model_dir)\n",
        "os.mkdir(config.log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541,
          "referenced_widgets": [
            "82ebc9a1d4b14260bdb2c22076312ead",
            "dd4e8020116c4053a9e97a3f5047a73d",
            "2df32672d4e34601b1d9e75004242a30",
            "c8b859c69e26482db4b7c71db0539bfa",
            "b83f322436004ebcafea351118d33a53",
            "6d1bce2d1af24eb8ade6137abc8d6b92",
            "199a910405a44c5e92ec25af29308577",
            "1b1bbaa156364d0a911e14d24ae0b126"
          ]
        },
        "id": "XnmU_JwJZCsl",
        "outputId": "e2802d7c-1fea-4090-b679-63fd7193cbe0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mniketpatel\u001b[0m (\u001b[33mniket\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/niket/grokking/grok-adversarial/wandb/run-20240529_172128-aah3x6lh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/niket/grok-adv-MLP-XOR/runs/aah3x6lh' target=\"_blank\">XOR-MLP-Wed_May_29_17:21:27_2024</a></strong> to <a href='https://wandb.ai/niket/grok-adv-MLP-XOR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/niket/grok-adv-MLP-XOR' target=\"_blank\">https://wandb.ai/niket/grok-adv-MLP-XOR</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/niket/grok-adv-MLP-XOR/runs/aah3x6lh' target=\"_blank\">https://wandb.ai/niket/grok-adv-MLP-XOR/runs/aah3x6lh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if config.wandb_log:\n",
        "  wandb_project = config.wandb_proj\n",
        "  wandb_run_name = f\"{config.wandb_pref}-{timestamp}\"\n",
        "  wandb.init(project=wandb_project, name=wandb_run_name, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "slXImW9bTPUm"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import einops\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.n_layers = len(hidden_dims)\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Define input layer\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
        "        self.input_activation = nn.ReLU()\n",
        "\n",
        "        # Define hidden layers\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
        "            self.hidden_layers.append(layer)\n",
        "            activation = nn.ReLU()\n",
        "            self.hidden_layers.append(activation)\n",
        "\n",
        "        # Define output layer\n",
        "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = einops.rearrange(x, \"b c u i -> b c (u i)\")\n",
        "        x = self.input_activation(self.input_layer(x))\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "        x = self.output_layer(x)\n",
        "        x = einops.rearrange(x, \"x 1 -> x\")\n",
        "        return x.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih0X2aJPUYpg",
        "outputId": "235e2588-8276-4ea4-bcaf-57368dfc9ac4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'input_dimension' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## create model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m MLP(input_dim \u001b[38;5;241m=\u001b[39m \u001b[43minput_dimension\u001b[49m, hidden_dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m50000\u001b[39m], output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m stats \u001b[38;5;241m=\u001b[39m train(model, loaders,\n\u001b[1;32m      5\u001b[0m       config\u001b[38;5;241m=\u001b[39mconfig, hulls\u001b[38;5;241m=\u001b[39mhulls,\n\u001b[1;32m      6\u001b[0m       add_hook_fn\u001b[38;5;241m=\u001b[39madd_hooks_MLP\n\u001b[1;32m      7\u001b[0m       )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'input_dimension' is not defined"
          ]
        }
      ],
      "source": [
        "## create model\n",
        "model = MLP(input_dim = config.input_dimension, hidden_dims = [config.hidden_dim], output_dim = 1)\n",
        "\n",
        "stats = train(model, loaders,\n",
        "      config=config, hulls=hulls,\n",
        "      add_hook_fn=add_hooks_MLP\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akGXvpIgQ74M"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "199a910405a44c5e92ec25af29308577": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b1bbaa156364d0a911e14d24ae0b126": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2df32672d4e34601b1d9e75004242a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_199a910405a44c5e92ec25af29308577",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b1bbaa156364d0a911e14d24ae0b126",
            "value": 1
          }
        },
        "6d1bce2d1af24eb8ade6137abc8d6b92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82ebc9a1d4b14260bdb2c22076312ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd4e8020116c4053a9e97a3f5047a73d",
              "IPY_MODEL_2df32672d4e34601b1d9e75004242a30"
            ],
            "layout": "IPY_MODEL_c8b859c69e26482db4b7c71db0539bfa"
          }
        },
        "b83f322436004ebcafea351118d33a53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8b859c69e26482db4b7c71db0539bfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd4e8020116c4053a9e97a3f5047a73d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b83f322436004ebcafea351118d33a53",
            "placeholder": "​",
            "style": "IPY_MODEL_6d1bce2d1af24eb8ade6137abc8d6b92",
            "value": "0.037 MB of 0.037 MB uploaded\r"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
